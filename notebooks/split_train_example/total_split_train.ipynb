{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split Data Training - Sequence LSTM + MLP 기반 CTR 예측\n",
        "\n",
        "10개로 분할된 데이터를 순차적으로 학습하여 하나의 통합 모델을 생성하는 코드입니다.\n",
        "\n",
        "## 주요 기능\n",
        "- 10개 split 데이터를 순차적으로 처리\n",
        "- 메모리 효율적 관리 (각 처리 후 메모리 해제)\n",
        "- models 폴더에 모델 저장\n",
        "- 증분 학습을 통한 통합 모델 생성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Using device: cuda\n",
            "⚙️  Enhanced Gradient Descent Configuration:\n",
            "   Batch Size: 4,096\n",
            "   Learning Rate: 1.0e-03\n",
            "   Weight Decay: 1.0e-05\n",
            "   Gradient Clipping: 1.0\n",
            "   Scheduler Min LR: 1.0e-06\n",
            "📁 Model directory: ../models/\n",
            "🎮 GPU: NVIDIA GeForce RTX 2070\n",
            "💾 GPU Memory: 8.6 GB\n"
          ]
        }
      ],
      "source": [
        "CFG = {\n",
        "    'BATCH_SIZE': 4096,\n",
        "    'EPOCHS_PER_SPLIT': 3,  # 각 split 데이터당 에포크 수\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'SEED': 42,\n",
        "    'DOWNSAMPLE_RATIO': 2,  # clicked=0 데이터를 clicked=1의 몇 배로 샘플링할지\n",
        "    'SPLIT_DATA_PATH': '../data/processed/split_data/',\n",
        "    'MODELS_PATH': '../models/',\n",
        "    'MODEL_NAME': 'ctr_lstm_mlp_model'\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# models 폴더가 없으면 생성\n",
        "os.makedirs(CFG['MODELS_PATH'], exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(CFG['SEED'])  # Seed 고정\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Management Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory - Allocated: 0.0MB, Cached: 0.0MB\n"
          ]
        }
      ],
      "source": [
        "def clear_memory():\n",
        "    \"\"\"메모리 정리를 위한 함수\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"현재 GPU 메모리 사용량 확인\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "        cached = torch.cuda.memory_reserved() / 1024**2  # MB\n",
        "        return f\"GPU Memory - Allocated: {allocated:.1f}MB, Cached: {cached:.1f}MB\"\n",
        "    return \"CPU mode - No GPU memory tracking\"\n",
        "\n",
        "print(get_memory_usage())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_downsample_data(file_path, downsample_ratio=2):\n",
        "    \"\"\"\n",
        "    Split 데이터를 로드하고 다운샘플링 수행\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "    \n",
        "    # 데이터 로드\n",
        "    df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    \n",
        "    # clicked == 1 데이터\n",
        "    clicked_1 = df[df['clicked'] == 1]\n",
        "    \n",
        "    # clicked == 0 데이터에서 동일 개수 x downsample_ratio 만큼 무작위 추출\n",
        "    clicked_0_count = len(clicked_1) * downsample_ratio\n",
        "    clicked_0 = df[df['clicked'] == 0]\n",
        "    \n",
        "    if len(clicked_0) < clicked_0_count:\n",
        "        # clicked=0 데이터가 부족한 경우 모든 데이터 사용\n",
        "        print(f\"Warning: Not enough clicked=0 data. Using all {len(clicked_0)} samples.\")\n",
        "        sampled_0 = clicked_0\n",
        "    else:\n",
        "        sampled_0 = clicked_0.sample(n=clicked_0_count, random_state=42)\n",
        "    \n",
        "    # 두 데이터프레임 합치기\n",
        "    result = pd.concat([clicked_1, sampled_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Downsampled shape: {result.shape}\")\n",
        "    print(f\"Clicked=0: {len(result[result['clicked']==0])}, Clicked=1: {len(result[result['clicked']==1])}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "def get_feature_columns(df):\n",
        "    \"\"\"피처 컬럼 추출\"\"\"\n",
        "    FEATURE_EXCLUDE = {\"clicked\", \"seq\", \"ID\"}\n",
        "    return [c for c in df.columns if c not in FEATURE_EXCLUDE]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset & DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClickDataset(Dataset):\n",
        "    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.feature_cols = feature_cols\n",
        "        self.seq_col = seq_col\n",
        "        self.target_col = target_col\n",
        "        self.has_target = has_target\n",
        "\n",
        "        # 비-시퀀스 피처: 전부 연속값으로\n",
        "        self.X = self.df[self.feature_cols].astype(float).fillna(0).values\n",
        "\n",
        "        # 시퀀스: 문자열 그대로 보관 (lazy 파싱)\n",
        "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
        "\n",
        "        if self.has_target:\n",
        "            self.y = self.df[self.target_col].astype(np.float32).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float)\n",
        "\n",
        "        # 전체 시퀀스 사용 (빈 시퀀스만 방어)\n",
        "        s = self.seq_strings[idx]\n",
        "        if s and s != 'nan':\n",
        "            try:\n",
        "                arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
        "            except:\n",
        "                arr = np.array([], dtype=np.float32)\n",
        "        else:\n",
        "            arr = np.array([], dtype=np.float32)\n",
        "\n",
        "        if arr.size == 0:\n",
        "            arr = np.array([0.0], dtype=np.float32)  # 빈 시퀀스 방어\n",
        "\n",
        "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
        "\n",
        "        if self.has_target:\n",
        "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
        "            return x, seq, y\n",
        "        else:\n",
        "            return x, seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn_train(batch):\n",
        "    xs, seqs, ys = zip(*batch)\n",
        "    xs = torch.stack(xs)\n",
        "    ys = torch.stack(ys)\n",
        "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
        "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    seq_lengths = torch.clamp(seq_lengths, min=1)  # 빈 시퀀스 방지\n",
        "    return xs, seqs_padded, seq_lengths, ys\n",
        "\n",
        "def collate_fn_infer(batch):\n",
        "    xs, seqs = zip(*batch)\n",
        "    xs = torch.stack(xs)\n",
        "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
        "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
        "    return xs, seqs_padded, seq_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TabularSeqModel(nn.Module):\n",
        "    def __init__(self, d_features, lstm_hidden=32, hidden_units=[1024, 512, 256, 128], dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 모든 비-시퀀스 피처에 BN\n",
        "        self.bn_x = nn.BatchNorm1d(d_features)\n",
        "        # seq: 숫자 시퀀스 → LSTM\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden, batch_first=True)\n",
        "\n",
        "        # 최종 MLP\n",
        "        input_dim = d_features + lstm_hidden\n",
        "        layers = []\n",
        "        for h in hidden_units:\n",
        "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            input_dim = h\n",
        "        layers += [nn.Linear(input_dim, 1)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x_feats, x_seq, seq_lengths):\n",
        "        # 비-시퀀스 피처\n",
        "        x = self.bn_x(x_feats)\n",
        "\n",
        "        # 시퀀스 → LSTM (pack)\n",
        "        x_seq = x_seq.unsqueeze(-1)  # (B, L, 1)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        h = h_n[-1]                  # (B, lstm_hidden)\n",
        "\n",
        "        z = torch.cat([x, h], dim=1)\n",
        "        return self.mlp(z).squeeze(1)  # logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Save & Load Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, model_path, model_config=None):\n",
        "    \"\"\"모델과 설정 저장\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': model_config,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, model_path)\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "def load_model(model_path, d_features, device='cpu'):\n",
        "    \"\"\"저장된 모델 로드\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    \n",
        "    # 모델 설정이 저장되어 있으면 사용, 없으면 기본값\n",
        "    if 'model_config' in checkpoint and checkpoint['model_config']:\n",
        "        config = checkpoint['model_config']\n",
        "        model = TabularSeqModel(\n",
        "            d_features=d_features,\n",
        "            lstm_hidden=config.get('lstm_hidden', 64),\n",
        "            hidden_units=config.get('hidden_units', [256, 128]),\n",
        "            dropout=config.get('dropout', 0.2)\n",
        "        )\n",
        "    else:\n",
        "        # 기본 설정\n",
        "        model = TabularSeqModel(\n",
        "            d_features=d_features,\n",
        "            lstm_hidden=64,\n",
        "            hidden_units=[256, 128],\n",
        "            dropout=0.2\n",
        "        )\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from: {model_path}\")\n",
        "    \n",
        "    if 'timestamp' in checkpoint:\n",
        "        print(f\"Model timestamp: {checkpoint['timestamp']}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_on_split(model, train_df, feature_cols, seq_col, target_col, \n",
        "                   batch_size=512, epochs=3, lr=1e-3, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    단일 split 데이터에 대해 모델 학습\n",
        "    \"\"\"\n",
        "    print(f\"Training on split data with {len(train_df)} samples\")\n",
        "    \n",
        "    # Train/Validation split\n",
        "    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
        "    print(f\"Train: {len(tr_df)}, Validation: {len(va_df)}\")\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_dataset = ClickDataset(tr_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "    val_dataset = ClickDataset(va_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
        "\n",
        "    # Loss & Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_batches = 0\n",
        "        \n",
        "        for xs, seqs, seq_lens, ys in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
        "            xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xs, seqs, seq_lens)\n",
        "            loss = criterion(logits, ys)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for xs, seqs, seq_lens, ys in tqdm(val_loader, desc=f\"Val Epoch {epoch}\"):\n",
        "                xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "                \n",
        "                logits = model(xs, seqs, seq_lens)\n",
        "                loss = criterion(logits, ys)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        \n",
        "        # Memory monitoring\n",
        "        print(f\"Memory usage: {get_memory_usage()}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all_splits():\n",
        "    \"\"\"\n",
        "    모든 split 데이터를 순차적으로 처리하여 통합 모델 학습\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Starting Split Data Training\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Split 파일 목록 가져오기\n",
        "    split_files = sorted(glob.glob(os.path.join(CFG['SPLIT_DATA_PATH'], \"part_*.parquet\")))\n",
        "    print(f\"Found {len(split_files)} split files:\")\n",
        "    for f in split_files:\n",
        "        print(f\"  - {os.path.basename(f)}\")\n",
        "    \n",
        "    if len(split_files) == 0:\n",
        "        print(\"No split files found!\")\n",
        "        return None\n",
        "    \n",
        "    # 첫 번째 파일로 feature 정보 확인\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Analyzing first split for feature info...\")\n",
        "    first_df = load_and_downsample_data(split_files[0], CFG['DOWNSAMPLE_RATIO'])\n",
        "    feature_cols = get_feature_columns(first_df)\n",
        "    seq_col = \"seq\"\n",
        "    target_col = \"clicked\"\n",
        "    \n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "    print(f\"Sequence column: {seq_col}\")\n",
        "    print(f\"Target column: {target_col}\")\n",
        "    \n",
        "    # 메모리 정리\n",
        "    del first_df\n",
        "    clear_memory()\n",
        "    \n",
        "    # 모델 초기화\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Initializing model...\")\n",
        "    model_config = {\n",
        "        'lstm_hidden': 64,\n",
        "        'hidden_units': [256, 128],\n",
        "        'dropout': 0.2\n",
        "    }\n",
        "    \n",
        "    model = TabularSeqModel(\n",
        "        d_features=len(feature_cols),\n",
        "        lstm_hidden=model_config['lstm_hidden'],\n",
        "        hidden_units=model_config['hidden_units'],\n",
        "        dropout=model_config['dropout']\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Initial memory: {get_memory_usage()}\")\n",
        "    \n",
        "    # 각 split에 대해 순차적으로 학습\n",
        "    for i, split_file in enumerate(split_files, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing Split {i}/{len(split_files)}: {os.path.basename(split_file)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        try:\n",
        "            # 데이터 로드 및 전처리\n",
        "            split_df = load_and_downsample_data(split_file, CFG['DOWNSAMPLE_RATIO'])\n",
        "            \n",
        "            # 학습 수행\n",
        "            model = train_on_split(\n",
        "                model=model,\n",
        "                train_df=split_df,\n",
        "                feature_cols=feature_cols,\n",
        "                seq_col=seq_col,\n",
        "                target_col=target_col,\n",
        "                batch_size=CFG['BATCH_SIZE'],\n",
        "                epochs=CFG['EPOCHS_PER_SPLIT'],\n",
        "                lr=CFG['LEARNING_RATE'],\n",
        "                device=device\n",
        "            )\n",
        "            \n",
        "            # 중간 모델 저장 (선택적)\n",
        "            if i % 3 == 0:  # 3번째마다 중간 저장\n",
        "                checkpoint_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_checkpoint_split_{i:02d}.pth\")\n",
        "                save_model(model, checkpoint_path, model_config)\n",
        "            \n",
        "            print(f\"Completed split {i}/{len(split_files)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing split {i}: {str(e)}\")\n",
        "            continue\n",
        "            \n",
        "        finally:\n",
        "            # 메모리 정리\n",
        "            if 'split_df' in locals():\n",
        "                del split_df\n",
        "            clear_memory()\n",
        "            print(f\"Memory after cleanup: {get_memory_usage()}\")\n",
        "    \n",
        "    # 최종 모델 저장\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Saving Final Model\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    final_model_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_final.pth\")\n",
        "    save_model(model, final_model_path, model_config)\n",
        "    \n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Final model saved to: {final_model_path}\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "    return model, feature_cols\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started at: 2025-09-19 16:22:04.724861\n",
            "================================================================================\n",
            "🚀 Starting Enhanced Split Data Training with Advanced Gradient Descent\n",
            "================================================================================\n",
            "📁 Found 10 split files:\n",
            "   - part_01.parquet\n",
            "   - part_02.parquet\n",
            "   - part_03.parquet\n",
            "   - part_04.parquet\n",
            "   - part_05.parquet\n",
            "   - part_06.parquet\n",
            "   - part_07.parquet\n",
            "   - part_08.parquet\n",
            "   - part_09.parquet\n",
            "   - part_10.parquet\n",
            "\n",
            "==================================================\n",
            "🔍 Analyzing first split for feature info...\n",
            "Loading data from: ../data/processed/split_data\\part_01.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61524, 119)\n",
            "Clicked=0: 41016, Clicked=1: 20508\n",
            "📊 Number of features: 117\n",
            "📈 Sequence column: seq\n",
            "🎯 Target column: clicked\n",
            "\n",
            "==================================================\n",
            "🧠 Initializing model & optimization strategy...\n",
            "✅ Model initialized with 97,003 parameters\n",
            "⚙️  Optimizer: Adam with LR=1.0e-03, weight_decay=1e-5\n",
            "📈 Scheduler: CosineAnnealingWarmRestarts\n",
            "💾 Initial memory: GPU Memory - Allocated: 0.4MB, Cached: 2.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 1/10: part_01.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_01.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61524, 119)\n",
            "Clicked=0: 41016, Clicked=1: 20508\n",
            "🔄 Training on Split 1 with 61524 samples\n",
            "📊 Current learning rate: 0.001000\n",
            "   Train: 49219, Validation: 12305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 1: 100%|██████████| 13/13 [00:15<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 1] Train: 0.6361 | Val: 0.6328\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.2MB, Cached: 5500.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 2: 100%|██████████| 13/13 [00:16<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 2] Train: 0.6056 | Val: 0.6136\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.4MB, Cached: 8216.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 3: 100%|██████████| 13/13 [00:15<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 3] Train: 0.5997 | Val: 0.6024\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.2MB, Cached: 8216.0MB\n",
            "   📈 LR Scheduler: 0.001000 -> 0.000750\n",
            "✅ Split 1 completed! Final validation loss: 0.6024\n",
            "✅ Split 1/10 completed!\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 58.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 2/10: part_02.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_02.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (63441, 119)\n",
            "Clicked=0: 42294, Clicked=1: 21147\n",
            "🔄 Training on Split 2 with 63441 samples\n",
            "📊 Current learning rate: 0.000750\n",
            "   Train: 50752, Validation: 12689\n",
            "🔍 Initial validation loss: 0.6002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 1: 100%|██████████| 13/13 [00:18<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 1] Train: 0.5900 | Val: 0.5907\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 35.0MB, Cached: 10454.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 2: 100%|██████████| 13/13 [00:20<00:00,  1.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 2] Train: 0.5845 | Val: 0.5849\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 33.7MB, Cached: 15594.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 3: 100%|██████████| 13/13 [00:21<00:00,  1.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 3] Train: 0.5831 | Val: 0.5828\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 35.5MB, Cached: 15594.0MB\n",
            "   📈 LR Scheduler: 0.000750 -> 0.000251\n",
            "✅ Split 2 completed! Final validation loss: 0.5828\n",
            "✅ Split 2/10 completed!\n",
            "📈 Validation Loss Change: +3.25%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 3/10: part_03.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_03.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (63387, 119)\n",
            "Clicked=0: 42258, Clicked=1: 21129\n",
            "🔄 Training on Split 3 with 63387 samples\n",
            "📊 Current learning rate: 0.000251\n",
            "   Train: 50709, Validation: 12678\n",
            "🔍 Initial validation loss: 0.5835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 1: 100%|██████████| 13/13 [02:21<00:00, 10.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 1] Train: 0.5795 | Val: 0.5815\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 35.6MB, Cached: 10440.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 2: 100%|██████████| 13/13 [02:12<00:00, 10.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 2] Train: 0.5774 | Val: 0.5807\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 37.0MB, Cached: 10440.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 3: 100%|██████████| 13/13 [01:40<00:00,  7.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 3] Train: 0.5777 | Val: 0.5803\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 37.3MB, Cached: 10440.0MB\n",
            "   📈 LR Scheduler: 0.000251 -> 0.001000\n",
            "✅ Split 3 completed! Final validation loss: 0.5803\n",
            "💾 Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_03.pth\n",
            "✅ Split 3/10 completed!\n",
            "📈 Validation Loss Change: +0.42%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 62.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 4/10: part_04.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_04.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (57975, 119)\n",
            "Clicked=0: 38650, Clicked=1: 19325\n",
            "🔄 Training on Split 4 with 57975 samples\n",
            "📊 Current learning rate: 0.001000\n",
            "   Train: 46380, Validation: 11595\n",
            "🔍 Initial validation loss: 0.5766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 1: 100%|██████████| 12/12 [00:34<00:00,  2.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 1] Train: 0.5820 | Val: 0.5765\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 31.1MB, Cached: 10350.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 2: 100%|██████████| 12/12 [00:39<00:00,  3.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 2] Train: 0.5794 | Val: 0.5769\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 31.4MB, Cached: 10350.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 3: 100%|██████████| 12/12 [00:39<00:00,  3.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 3] Train: 0.5761 | Val: 0.5750\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 33.1MB, Cached: 10350.0MB\n",
            "   📈 LR Scheduler: 0.001000 -> 0.000750\n",
            "✅ Split 4 completed! Final validation loss: 0.5750\n",
            "✅ Split 4/10 completed!\n",
            "📈 Validation Loss Change: +0.92%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 58.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 5/10: part_05.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_05.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (57609, 119)\n",
            "Clicked=0: 38406, Clicked=1: 19203\n",
            "🔄 Training on Split 5 with 57609 samples\n",
            "📊 Current learning rate: 0.000750\n",
            "   Train: 46087, Validation: 11522\n",
            "🔍 Initial validation loss: 0.5761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 1: 100%|██████████| 12/12 [00:41<00:00,  3.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 1] Train: 0.5826 | Val: 0.5760\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.2MB, Cached: 13190.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 2: 100%|██████████| 12/12 [00:39<00:00,  3.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 2] Train: 0.5788 | Val: 0.5740\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.4MB, Cached: 15840.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 3: 100%|██████████| 12/12 [00:37<00:00,  3.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 3] Train: 0.5777 | Val: 0.5738\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 29.2MB, Cached: 15840.0MB\n",
            "   📈 LR Scheduler: 0.000750 -> 0.000251\n",
            "✅ Split 5 completed! Final validation loss: 0.5738\n",
            "✅ Split 5/10 completed!\n",
            "📈 Validation Loss Change: +0.21%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 6/10: part_06.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_06.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61506, 119)\n",
            "Clicked=0: 41004, Clicked=1: 20502\n",
            "🔄 Training on Split 6 with 61506 samples\n",
            "📊 Current learning rate: 0.000251\n",
            "   Train: 49204, Validation: 12302\n",
            "🔍 Initial validation loss: 0.5743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 1: 100%|██████████| 13/13 [00:35<00:00,  2.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 1] Train: 0.5710 | Val: 0.5757\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 9406.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 2: 100%|██████████| 13/13 [00:26<00:00,  2.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 2] Train: 0.5754 | Val: 0.5746\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 9406.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 3: 100%|██████████| 13/13 [00:25<00:00,  1.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 3] Train: 0.5754 | Val: 0.5745\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 12058.0MB\n",
            "   📈 LR Scheduler: 0.000251 -> 0.001000\n",
            "✅ Split 6 completed! Final validation loss: 0.5745\n",
            "💾 Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_06.pth\n",
            "✅ Split 6/10 completed!\n",
            "📈 Validation Loss Change: -0.12%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 7/10: part_07.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_07.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (62148, 119)\n",
            "Clicked=0: 41432, Clicked=1: 20716\n",
            "🔄 Training on Split 7 with 62148 samples\n",
            "📊 Current learning rate: 0.001000\n",
            "   Train: 49718, Validation: 12430\n",
            "🔍 Initial validation loss: 0.5585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 1: 100%|██████████| 13/13 [00:18<00:00,  1.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 1] Train: 0.5773 | Val: 0.5590\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 23.9MB, Cached: 7876.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 2: 100%|██████████| 13/13 [01:46<00:00,  8.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 2] Train: 0.5777 | Val: 0.5612\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 24.2MB, Cached: 10460.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 3: 100%|██████████| 13/13 [02:56<00:00, 13.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 3] Train: 0.5731 | Val: 0.5584\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 23.1MB, Cached: 10460.0MB\n",
            "   📈 LR Scheduler: 0.001000 -> 0.000750\n",
            "✅ Split 7 completed! Final validation loss: 0.5584\n",
            "✅ Split 7/10 completed!\n",
            "📈 Validation Loss Change: +2.80%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 8/10: part_08.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_08.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (62640, 119)\n",
            "Clicked=0: 41760, Clicked=1: 20880\n",
            "🔄 Training on Split 8 with 62640 samples\n",
            "📊 Current learning rate: 0.000750\n",
            "   Train: 50112, Validation: 12528\n",
            "🔍 Initial validation loss: 0.5577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 1: 100%|██████████| 13/13 [00:32<00:00,  2.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 1] Train: 0.5766 | Val: 0.5554\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.8MB, Cached: 9914.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 2: 100%|██████████| 13/13 [00:32<00:00,  2.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 2] Train: 0.5734 | Val: 0.5554\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.7MB, Cached: 14798.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 3: 100%|██████████| 13/13 [00:24<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 3] Train: 0.5723 | Val: 0.5537\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.5MB, Cached: 4654.0MB\n",
            "   📈 LR Scheduler: 0.000750 -> 0.000251\n",
            "✅ Split 8 completed! Final validation loss: 0.5537\n",
            "✅ Split 8/10 completed!\n",
            "📈 Validation Loss Change: +0.84%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 9/10: part_09.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_09.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (60402, 119)\n",
            "Clicked=0: 40268, Clicked=1: 20134\n",
            "🔄 Training on Split 9 with 60402 samples\n",
            "📊 Current learning rate: 0.000251\n",
            "   Train: 48321, Validation: 12081\n",
            "🔍 Initial validation loss: 0.5718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 1: 100%|██████████| 12/12 [01:51<00:00,  9.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 1] Train: 0.5751 | Val: 0.5726\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 62.6MB, Cached: 15714.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 2: 100%|██████████| 12/12 [04:14<00:00, 21.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 2] Train: 0.5732 | Val: 0.5715\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 55.9MB, Cached: 15714.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 3: 100%|██████████| 12/12 [04:28<00:00, 22.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 3] Train: 0.5735 | Val: 0.5713\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 62.6MB, Cached: 18366.0MB\n",
            "   📈 LR Scheduler: 0.000251 -> 0.001000\n",
            "✅ Split 9 completed! Final validation loss: 0.5713\n",
            "💾 Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_09.pth\n",
            "✅ Split 9/10 completed!\n",
            "📈 Validation Loss Change: -3.18%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "🎯 Processing Split 10/10: part_10.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_10.parquet\n",
            "Original shape: (1064179, 119)\n",
            "Downsampled shape: (61575, 119)\n",
            "Clicked=0: 41050, Clicked=1: 20525\n",
            "🔄 Training on Split 10 with 61575 samples\n",
            "📊 Current learning rate: 0.001000\n",
            "   Train: 49260, Validation: 12315\n",
            "🔍 Initial validation loss: 0.5921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 1: 100%|██████████| 13/13 [00:24<00:00,  1.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 1] Train: 0.5700 | Val: 0.5916\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.6MB, Cached: 11880.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 2: 100%|██████████| 13/13 [00:30<00:00,  2.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 2] Train: 0.5695 | Val: 0.5884\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.7MB, Cached: 11880.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 3: 100%|██████████| 13/13 [00:35<00:00,  2.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 3] Train: 0.5718 | Val: 0.5949\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.7MB, Cached: 11880.0MB\n",
            "   📈 LR Scheduler: 0.001000 -> 0.000750\n",
            "✅ Split 10 completed! Final validation loss: 0.5949\n",
            "✅ Split 10/10 completed!\n",
            "📈 Validation Loss Change: -4.13%\n",
            "🧹 Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "📊 Training Summary\n",
            "================================================================================\n",
            "Split-wise Performance:\n",
            "   Split  1: Val Loss = 0.6024, LR = 0.000750\n",
            "   Split  2: Val Loss = 0.5828, LR = 0.000251\n",
            "   Split  3: Val Loss = 0.5803, LR = 0.001000\n",
            "   Split  4: Val Loss = 0.5750, LR = 0.000750\n",
            "   Split  5: Val Loss = 0.5738, LR = 0.000251\n",
            "   Split  6: Val Loss = 0.5745, LR = 0.001000\n",
            "   Split  7: Val Loss = 0.5584, LR = 0.000750\n",
            "   Split  8: Val Loss = 0.5537, LR = 0.000251\n",
            "   Split  9: Val Loss = 0.5713, LR = 0.001000\n",
            "   Split 10: Val Loss = 0.5949, LR = 0.000750\n",
            "\n",
            "🎯 Overall Improvement: +1.24%\n",
            "📈 Initial Loss: 0.6024 → Final Loss: 0.5949\n",
            "\n",
            "============================================================\n",
            "💾 Saving Final Model with Full State\n",
            "============================================================\n",
            "✅ Training completed successfully!\n",
            "💾 Final model saved to: ../models/ctr_lstm_mlp_enhanced_final.pth\n",
            "🧠 Final memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "🎯 Final learning rate: 0.000750\n",
            "\n",
            "Training completed!\n",
            "Training time: 0:38:14.215847\n",
            "Final memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "# 모든 split 데이터로 학습 실행\n",
        "start_time = datetime.now()\n",
        "print(f\"Training started at: {start_time}\")\n",
        "\n",
        "try:\n",
        "    trained_model, feature_columns = train_all_splits()\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Training time: {training_time}\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Training failed with error: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n",
            "Test data shape: (1527298, 119)\n",
            "Test data columns: ['ID', 'gender', 'age_group', 'inventory_id', 'day_of_week', 'hour', 'seq', 'l_feat_1', 'l_feat_2', 'l_feat_3', 'l_feat_4', 'l_feat_5', 'l_feat_6', 'l_feat_7', 'l_feat_8', 'l_feat_9', 'l_feat_10', 'l_feat_11', 'l_feat_12', 'l_feat_13', 'l_feat_14', 'l_feat_15', 'l_feat_16', 'l_feat_17', 'l_feat_18', 'l_feat_19', 'l_feat_20', 'l_feat_21', 'l_feat_22', 'l_feat_23', 'l_feat_24', 'l_feat_25', 'l_feat_26', 'l_feat_27', 'feat_e_1', 'feat_e_2', 'feat_e_3', 'feat_e_4', 'feat_e_5', 'feat_e_6', 'feat_e_7', 'feat_e_8', 'feat_e_9', 'feat_e_10', 'feat_d_1', 'feat_d_2', 'feat_d_3', 'feat_d_4', 'feat_d_5', 'feat_d_6', 'feat_c_1', 'feat_c_2', 'feat_c_3', 'feat_c_4', 'feat_c_5', 'feat_c_6', 'feat_c_7', 'feat_c_8', 'feat_b_1', 'feat_b_2', 'feat_b_3', 'feat_b_4', 'feat_b_5', 'feat_b_6', 'feat_a_1', 'feat_a_2', 'feat_a_3', 'feat_a_4', 'feat_a_5', 'feat_a_6', 'feat_a_7', 'feat_a_8', 'feat_a_9', 'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_14', 'feat_a_15', 'feat_a_16', 'feat_a_17', 'feat_a_18', 'history_a_1', 'history_a_2', 'history_a_3', 'history_a_4', 'history_a_5', 'history_a_6', 'history_a_7', 'history_b_1', 'history_b_2', 'history_b_3', 'history_b_4', 'history_b_5', 'history_b_6', 'history_b_7', 'history_b_8', 'history_b_9', 'history_b_10', 'history_b_11', 'history_b_12', 'history_b_13', 'history_b_14', 'history_b_15', 'history_b_16', 'history_b_17', 'history_b_18', 'history_b_19', 'history_b_20', 'history_b_21', 'history_b_22', 'history_b_23', 'history_b_24', 'history_b_25', 'history_b_26', 'history_b_27', 'history_b_28', 'history_b_29', 'history_b_30']\n",
            "Test data shape after removing ID: (1527298, 118)\n",
            "Memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터 로드\n",
        "print(\"Loading test data...\")\n",
        "test_df = pd.read_parquet(\"../data/raw/test.parquet\", engine=\"pyarrow\")\n",
        "\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "\n",
        "# ID 컬럼 따로 보관 (제출용)\n",
        "test_ids = test_df['ID'].copy()\n",
        "\n",
        "# ID 컬럼 제거\n",
        "test_df = test_df.drop(columns=['ID'])\n",
        "\n",
        "print(f\"Test data shape after removing ID: {test_df.shape}\")\n",
        "print(f\"Memory usage: {get_memory_usage()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with Saved Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_inference(model_path, test_df, feature_cols, batch_size=4096):\n",
        "    \"\"\"\n",
        "    저장된 모델을 로드하여 테스트 데이터에 대해 추론 수행\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    \n",
        "    # 모델 로드\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file not found: {model_path}\")\n",
        "        return None\n",
        "        \n",
        "    model = load_model(model_path, len(feature_cols), device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded successfully!\")\n",
        "    print(f\"Memory after model loading: {get_memory_usage()}\")\n",
        "    \n",
        "    # 테스트 데이터셋 생성\n",
        "    seq_col = \"seq\"\n",
        "    test_dataset = ClickDataset(test_df, feature_cols, seq_col, has_target=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_infer)\n",
        "    \n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "    print(f\"Number of batches: {len(test_loader)}\")\n",
        "    \n",
        "    # 추론 수행\n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xs, seqs, lens) in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
        "            xs, seqs, lens = xs.to(device), seqs.to(device), lens.to(device)\n",
        "            \n",
        "            # 모델 예측\n",
        "            logits = model(xs, seqs, lens)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            \n",
        "            predictions.append(probs.cpu())\n",
        "            \n",
        "            # 메모리 정리 (큰 배치 처리 시)\n",
        "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                clear_memory()\n",
        "    \n",
        "    # 예측 결과 합치기\n",
        "    final_predictions = torch.cat(predictions).numpy()\n",
        "    \n",
        "    print(f\"Inference completed!\")\n",
        "    print(f\"Predictions shape: {final_predictions.shape}\")\n",
        "    print(f\"Prediction range: [{final_predictions.min():.4f}, {final_predictions.max():.4f}]\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "    # 메모리 정리\n",
        "    del model\n",
        "    clear_memory()\n",
        "    \n",
        "    return final_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🔮 STARTING ENHANCED INFERENCE WITH TRAINED MODEL\n",
            "================================================================================\n",
            "✅ Using feature columns from training session: 117 features\n",
            "🎯 Final feature count: 117\n",
            "============================================================\n",
            "🔮 Starting Model Inference\n",
            "============================================================\n",
            "🔄 Loading model from: ../models/ctr_lstm_mlp_enhanced_final.pth\n",
            "📊 Features from checkpoint: 117\n",
            "🏗️  Model config: LSTM=64, Hidden=[256, 128]\n",
            "⏰ Model timestamp: 2025-09-19T17:00:18.930169\n",
            "📚 Trained on 10 splits\n",
            "🎯 Final validation loss: 0.5949\n",
            "✅ Model loaded successfully!\n",
            "✅ Using feature columns from checkpoint: 117 features\n",
            "💾 Memory after model loading: GPU Memory - Allocated: 17.4MB, Cached: 60.0MB\n",
            "📊 Test dataset size: 1,527,298\n",
            "📦 Number of batches: 373\n",
            "🔧 Batch size: 4,096\n",
            "\n",
            "🚀 Starting inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔮 Inference Progress:  27%|██▋       | 101/373 [01:36<04:17,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   📊 Processed 100 batches | Avg batch time: 0.143s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔮 Inference Progress:  54%|█████▍    | 201/373 [03:10<02:53,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   📊 Processed 200 batches | Avg batch time: 0.143s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔮 Inference Progress:  81%|████████  | 301/373 [04:42<01:08,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   📊 Processed 300 batches | Avg batch time: 0.137s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔮 Inference Progress: 100%|██████████| 373/373 [05:46<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Inference completed successfully!\n",
            "⏱️  Total inference time: 346.79 seconds\n",
            "📊 Average batch time: 0.141 seconds\n",
            "🔢 Predictions shape: (1527298,)\n",
            "📈 Prediction statistics:\n",
            "   Min: 0.004356\n",
            "   Max: 1.000000\n",
            "   Mean: 0.304236\n",
            "   Std: 0.131825\n",
            "   Median: 0.291847\n",
            "📊 Prediction distribution:\n",
            "   0.0-0.1: 27,535 (1.8%)\n",
            "   0.1-0.2: 363,634 (23.8%)\n",
            "   0.2-0.3: 407,705 (26.7%)\n",
            "   0.3-0.4: 377,544 (24.7%)\n",
            "   0.4-0.5: 216,029 (14.1%)\n",
            "   0.5-0.6: 102,595 (6.7%)\n",
            "   0.6-0.7: 27,081 (1.8%)\n",
            "   0.7-0.8: 4,295 (0.3%)\n",
            "   0.8-0.9: 698 (0.0%)\n",
            "   0.9-1.0: 182 (0.0%)\n",
            "💾 Final memory usage: GPU Memory - Allocated: 71.2MB, Cached: 3230.0MB\n"
          ]
        }
      ],
      "source": [
        "# 저장된 모델로 추론 수행\n",
        "final_model_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_final.pth\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting Inference\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 학습이 완료된 경우 feature_columns 사용, 아니면 모델에서 추출\n",
        "if 'feature_columns' in locals() and feature_columns is not None:\n",
        "    inference_feature_cols = feature_columns\n",
        "else:\n",
        "    # 첫 번째 split 파일에서 feature 정보 추출\n",
        "    print(\"Extracting feature information from split data...\")\n",
        "    split_files = sorted(glob.glob(os.path.join(CFG['SPLIT_DATA_PATH'], \"part_*.parquet\")))\n",
        "    if len(split_files) > 0:\n",
        "        temp_df = pd.read_parquet(split_files[0], engine=\"pyarrow\", nrows=1000)  # 작은 샘플만\n",
        "        inference_feature_cols = get_feature_columns(temp_df)\n",
        "        del temp_df\n",
        "        clear_memory()\n",
        "    else:\n",
        "        print(\"Error: No split files found for feature extraction!\")\n",
        "        raise FileNotFoundError(\"Split files not found\")\n",
        "\n",
        "print(f\"Using {len(inference_feature_cols)} features for inference\")\n",
        "\n",
        "# 추론 실행\n",
        "test_predictions = perform_inference(\n",
        "    model_path=final_model_path,\n",
        "    test_df=test_df,\n",
        "    feature_cols=inference_feature_cols,\n",
        "    batch_size=CFG['BATCH_SIZE']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "📋 Creating Submission File (베이스라인 호환 양식)\n",
            "============================================================\n",
            "Sample submission shape: (1527298, 2)\n",
            "Submission predictions stats:\n",
            "  Min: 0.004356\n",
            "  Max: 1.000000\n",
            "  Mean: 0.304236\n",
            "  Std: 0.131825\n",
            "📁 Found 2 existing submission files. Next: submission_3.csv\n",
            "💾 Submission file saved: ../outputs\\submission_3.csv\n",
            "📊 Submission shape: (1527298, 2)\n",
            "📁 File: submission_3.csv\n",
            "🧹 Final memory usage: GPU Memory - Allocated: 17.0MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "if test_predictions is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"📋 Creating Submission File (베이스라인 호환 양식)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # 기존 sample_submission.csv 파일 읽기\n",
        "    sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
        "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
        "    \n",
        "    # 예측 결과로 clicked 컬럼 업데이트\n",
        "    submission_df = sample_submission.copy()\n",
        "    submission_df['clicked'] = test_predictions\n",
        "    \n",
        "    # 결과 확인\n",
        "    print(f\"Submission predictions stats:\")\n",
        "    print(f\"  Min: {test_predictions.min():.6f}\")\n",
        "    print(f\"  Max: {test_predictions.max():.6f}\")\n",
        "    print(f\"  Mean: {test_predictions.mean():.6f}\")\n",
        "    print(f\"  Std: {test_predictions.std():.6f}\")\n",
        "    \n",
        "    # outputs 폴더 확인 및 생성\n",
        "    output_dir = '../outputs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # 기존 제출 파일 확인하여 번호 결정 (베이스라인과 동일한 방식)\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith('submission_') and f.endswith('.csv')]\n",
        "    \n",
        "    if len(existing_files) == 0:\n",
        "        next_num = 1\n",
        "        print(\"📁 No existing submission files found. Starting with submission_1.csv\")\n",
        "    else:\n",
        "        nums = [int(f.split('_')[1].split('.')[0]) for f in existing_files]\n",
        "        next_num = max(nums) + 1\n",
        "        print(f\"📁 Found {len(existing_files)} existing submission files. Next: submission_{next_num}.csv\")\n",
        "    \n",
        "    # 새로운 파일명으로 저장 (베이스라인과 동일한 양식)\n",
        "    output_path = os.path.join(output_dir, f'submission_{next_num}.csv')\n",
        "    \n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"💾 Submission file saved: {output_path}\")\n",
        "    print(f\"📊 Submission shape: {submission_df.shape}\")\n",
        "    print(f\"📁 File: submission_{next_num}.csv\")\n",
        "    \n",
        "    # 최종 메모리 정리\n",
        "    clear_memory()\n",
        "    print(f\"🧹 Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Error: No predictions available for submission!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "이 노트북은 다음과 같은 작업을 수행합니다:\n",
        "\n",
        "### 📊 **주요 기능**\n",
        "1. **Split 데이터 순차 처리**: 10개로 분할된 데이터를 메모리 효율적으로 순차 학습\n",
        "2. **메모리 관리**: 각 split 처리 후 자동 메모리 해제 및 모니터링\n",
        "3. **증분 학습**: 이전 학습 결과를 바탕으로 다음 split 데이터 학습\n",
        "4. **모델 저장**: models 폴더에 체크포인트 및 최종 모델 저장\n",
        "5. **자동화된 추론**: 저장된 모델로 테스트 데이터 예측\n",
        "\n",
        "### 🔧 **기술적 특징**\n",
        "- **LSTM + MLP 하이브리드 모델**: 시퀀스와 테이블 피처를 동시 처리\n",
        "- **다운샘플링**: 불균형 데이터 해결 (clicked=0 : clicked=1 = 2:1 비율)\n",
        "- **배치 처리**: 대용량 데이터를 위한 효율적 배치 처리\n",
        "- **GPU 메모리 최적화**: CUDA 메모리 캐시 정리 및 모니터링\n",
        "\n",
        "### 📁 **출력 파일**\n",
        "- **모델**: `../models/ctr_lstm_mlp_model_final.pth`\n",
        "- **체크포인트**: `../models/ctr_lstm_mlp_model_checkpoint_split_XX.pth` (3번째마다)\n",
        "- **제출 파일**: `../outputs/submission_split_training_XXX_YYYYMMDD_HHMMSS.csv`\n",
        "\n",
        "### ⚡ **성능 최적화**\n",
        "- 메모리 사용량 실시간 모니터링\n",
        "- 각 split 처리 후 자동 가비지 컬렉션\n",
        "- 배치별 메모리 정리 (100배치마다)\n",
        "- PyTorch CUDA 캐시 정리\n",
        "\n",
        "이 방식으로 대용량 데이터도 메모리 제한 없이 안정적으로 학습할 수 있습니다! 🚀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dacon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split Data Training - Sequence LSTM + MLP ê¸°ë°˜ CTR ì˜ˆì¸¡\n",
        "\n",
        "10ê°œë¡œ ë¶„í• ëœ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ í•˜ë‚˜ì˜ í†µí•© ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” ê¸°ëŠ¥\n",
        "- 10ê°œ split ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬\n",
        "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê´€ë¦¬ (ê° ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ í•´ì œ)\n",
        "- models í´ë”ì— ëª¨ë¸ ì €ì¥\n",
        "- ì¦ë¶„ í•™ìŠµì„ í†µí•œ í†µí•© ëª¨ë¸ ìƒì„±\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Using device: cuda\n",
            "âš™ï¸  Enhanced Gradient Descent Configuration:\n",
            "   Batch Size: 4,096\n",
            "   Learning Rate: 1.0e-03\n",
            "   Weight Decay: 1.0e-05\n",
            "   Gradient Clipping: 1.0\n",
            "   Scheduler Min LR: 1.0e-06\n",
            "ğŸ“ Model directory: ../models/\n",
            "ğŸ® GPU: NVIDIA GeForce RTX 2070\n",
            "ğŸ’¾ GPU Memory: 8.6 GB\n"
          ]
        }
      ],
      "source": [
        "CFG = {\n",
        "    'BATCH_SIZE': 4096,\n",
        "    'EPOCHS_PER_SPLIT': 3,  # ê° split ë°ì´í„°ë‹¹ ì—í¬í¬ ìˆ˜\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'SEED': 42,\n",
        "    'DOWNSAMPLE_RATIO': 2,  # clicked=0 ë°ì´í„°ë¥¼ clicked=1ì˜ ëª‡ ë°°ë¡œ ìƒ˜í”Œë§í• ì§€\n",
        "    'SPLIT_DATA_PATH': '../data/processed/split_data/',\n",
        "    'MODELS_PATH': '../models/',\n",
        "    'MODEL_NAME': 'ctr_lstm_mlp_model'\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# models í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
        "os.makedirs(CFG['MODELS_PATH'], exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(CFG['SEED'])  # Seed ê³ ì •\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Management Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory - Allocated: 0.0MB, Cached: 0.0MB\n"
          ]
        }
      ],
      "source": [
        "def clear_memory():\n",
        "    \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "        cached = torch.cuda.memory_reserved() / 1024**2  # MB\n",
        "        return f\"GPU Memory - Allocated: {allocated:.1f}MB, Cached: {cached:.1f}MB\"\n",
        "    return \"CPU mode - No GPU memory tracking\"\n",
        "\n",
        "print(get_memory_usage())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_downsample_data(file_path, downsample_ratio=2):\n",
        "    \"\"\"\n",
        "    Split ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë‹¤ìš´ìƒ˜í”Œë§ ìˆ˜í–‰\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "    \n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
        "    print(f\"Original shape: {df.shape}\")\n",
        "    \n",
        "    # clicked == 1 ë°ì´í„°\n",
        "    clicked_1 = df[df['clicked'] == 1]\n",
        "    \n",
        "    # clicked == 0 ë°ì´í„°ì—ì„œ ë™ì¼ ê°œìˆ˜ x downsample_ratio ë§Œí¼ ë¬´ì‘ìœ„ ì¶”ì¶œ\n",
        "    clicked_0_count = len(clicked_1) * downsample_ratio\n",
        "    clicked_0 = df[df['clicked'] == 0]\n",
        "    \n",
        "    if len(clicked_0) < clicked_0_count:\n",
        "        # clicked=0 ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ëª¨ë“  ë°ì´í„° ì‚¬ìš©\n",
        "        print(f\"Warning: Not enough clicked=0 data. Using all {len(clicked_0)} samples.\")\n",
        "        sampled_0 = clicked_0\n",
        "    else:\n",
        "        sampled_0 = clicked_0.sample(n=clicked_0_count, random_state=42)\n",
        "    \n",
        "    # ë‘ ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°\n",
        "    result = pd.concat([clicked_1, sampled_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Downsampled shape: {result.shape}\")\n",
        "    print(f\"Clicked=0: {len(result[result['clicked']==0])}, Clicked=1: {len(result[result['clicked']==1])}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "def get_feature_columns(df):\n",
        "    \"\"\"í”¼ì²˜ ì»¬ëŸ¼ ì¶”ì¶œ\"\"\"\n",
        "    FEATURE_EXCLUDE = {\"clicked\", \"seq\", \"ID\"}\n",
        "    return [c for c in df.columns if c not in FEATURE_EXCLUDE]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset & DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClickDataset(Dataset):\n",
        "    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.feature_cols = feature_cols\n",
        "        self.seq_col = seq_col\n",
        "        self.target_col = target_col\n",
        "        self.has_target = has_target\n",
        "\n",
        "        # ë¹„-ì‹œí€€ìŠ¤ í”¼ì²˜: ì „ë¶€ ì—°ì†ê°’ìœ¼ë¡œ\n",
        "        self.X = self.df[self.feature_cols].astype(float).fillna(0).values\n",
        "\n",
        "        # ì‹œí€€ìŠ¤: ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë³´ê´€ (lazy íŒŒì‹±)\n",
        "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
        "\n",
        "        if self.has_target:\n",
        "            self.y = self.df[self.target_col].astype(np.float32).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float)\n",
        "\n",
        "        # ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš© (ë¹ˆ ì‹œí€€ìŠ¤ë§Œ ë°©ì–´)\n",
        "        s = self.seq_strings[idx]\n",
        "        if s and s != 'nan':\n",
        "            try:\n",
        "                arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
        "            except:\n",
        "                arr = np.array([], dtype=np.float32)\n",
        "        else:\n",
        "            arr = np.array([], dtype=np.float32)\n",
        "\n",
        "        if arr.size == 0:\n",
        "            arr = np.array([0.0], dtype=np.float32)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì–´\n",
        "\n",
        "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
        "\n",
        "        if self.has_target:\n",
        "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
        "            return x, seq, y\n",
        "        else:\n",
        "            return x, seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn_train(batch):\n",
        "    xs, seqs, ys = zip(*batch)\n",
        "    xs = torch.stack(xs)\n",
        "    ys = torch.stack(ys)\n",
        "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
        "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€\n",
        "    return xs, seqs_padded, seq_lengths, ys\n",
        "\n",
        "def collate_fn_infer(batch):\n",
        "    xs, seqs = zip(*batch)\n",
        "    xs = torch.stack(xs)\n",
        "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
        "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
        "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
        "    return xs, seqs_padded, seq_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TabularSeqModel(nn.Module):\n",
        "    def __init__(self, d_features, lstm_hidden=32, hidden_units=[1024, 512, 256, 128], dropout=0.2):\n",
        "        super().__init__()\n",
        "        # ëª¨ë“  ë¹„-ì‹œí€€ìŠ¤ í”¼ì²˜ì— BN\n",
        "        self.bn_x = nn.BatchNorm1d(d_features)\n",
        "        # seq: ìˆ«ì ì‹œí€€ìŠ¤ â†’ LSTM\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden, batch_first=True)\n",
        "\n",
        "        # ìµœì¢… MLP\n",
        "        input_dim = d_features + lstm_hidden\n",
        "        layers = []\n",
        "        for h in hidden_units:\n",
        "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
        "            input_dim = h\n",
        "        layers += [nn.Linear(input_dim, 1)]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x_feats, x_seq, seq_lengths):\n",
        "        # ë¹„-ì‹œí€€ìŠ¤ í”¼ì²˜\n",
        "        x = self.bn_x(x_feats)\n",
        "\n",
        "        # ì‹œí€€ìŠ¤ â†’ LSTM (pack)\n",
        "        x_seq = x_seq.unsqueeze(-1)  # (B, L, 1)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            x_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        h = h_n[-1]                  # (B, lstm_hidden)\n",
        "\n",
        "        z = torch.cat([x, h], dim=1)\n",
        "        return self.mlp(z).squeeze(1)  # logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Save & Load Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, model_path, model_config=None):\n",
        "    \"\"\"ëª¨ë¸ê³¼ ì„¤ì • ì €ì¥\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': model_config,\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    torch.save(checkpoint, model_path)\n",
        "    print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "def load_model(model_path, d_features, device='cpu'):\n",
        "    \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    \n",
        "    # ëª¨ë¸ ì„¤ì •ì´ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’\n",
        "    if 'model_config' in checkpoint and checkpoint['model_config']:\n",
        "        config = checkpoint['model_config']\n",
        "        model = TabularSeqModel(\n",
        "            d_features=d_features,\n",
        "            lstm_hidden=config.get('lstm_hidden', 64),\n",
        "            hidden_units=config.get('hidden_units', [256, 128]),\n",
        "            dropout=config.get('dropout', 0.2)\n",
        "        )\n",
        "    else:\n",
        "        # ê¸°ë³¸ ì„¤ì •\n",
        "        model = TabularSeqModel(\n",
        "            d_features=d_features,\n",
        "            lstm_hidden=64,\n",
        "            hidden_units=[256, 128],\n",
        "            dropout=0.2\n",
        "        )\n",
        "    \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from: {model_path}\")\n",
        "    \n",
        "    if 'timestamp' in checkpoint:\n",
        "        print(f\"Model timestamp: {checkpoint['timestamp']}\")\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_on_split(model, train_df, feature_cols, seq_col, target_col, \n",
        "                   batch_size=512, epochs=3, lr=1e-3, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ split ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ\n",
        "    \"\"\"\n",
        "    print(f\"Training on split data with {len(train_df)} samples\")\n",
        "    \n",
        "    # Train/Validation split\n",
        "    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
        "    print(f\"Train: {len(tr_df)}, Validation: {len(va_df)}\")\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_dataset = ClickDataset(tr_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "    val_dataset = ClickDataset(va_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
        "\n",
        "    # Loss & Optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_batches = 0\n",
        "        \n",
        "        for xs, seqs, seq_lens, ys in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
        "            xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xs, seqs, seq_lens)\n",
        "            loss = criterion(logits, ys)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for xs, seqs, seq_lens, ys in tqdm(val_loader, desc=f\"Val Epoch {epoch}\"):\n",
        "                xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "                \n",
        "                logits = model(xs, seqs, seq_lens)\n",
        "                loss = criterion(logits, ys)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        \n",
        "        # Memory monitoring\n",
        "        print(f\"Memory usage: {get_memory_usage()}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all_splits():\n",
        "    \"\"\"\n",
        "    ëª¨ë“  split ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í†µí•© ëª¨ë¸ í•™ìŠµ\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Starting Split Data Training\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Split íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
        "    split_files = sorted(glob.glob(os.path.join(CFG['SPLIT_DATA_PATH'], \"part_*.parquet\")))\n",
        "    print(f\"Found {len(split_files)} split files:\")\n",
        "    for f in split_files:\n",
        "        print(f\"  - {os.path.basename(f)}\")\n",
        "    \n",
        "    if len(split_files) == 0:\n",
        "        print(\"No split files found!\")\n",
        "        return None\n",
        "    \n",
        "    # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ feature ì •ë³´ í™•ì¸\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Analyzing first split for feature info...\")\n",
        "    first_df = load_and_downsample_data(split_files[0], CFG['DOWNSAMPLE_RATIO'])\n",
        "    feature_cols = get_feature_columns(first_df)\n",
        "    seq_col = \"seq\"\n",
        "    target_col = \"clicked\"\n",
        "    \n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "    print(f\"Sequence column: {seq_col}\")\n",
        "    print(f\"Target column: {target_col}\")\n",
        "    \n",
        "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "    del first_df\n",
        "    clear_memory()\n",
        "    \n",
        "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"Initializing model...\")\n",
        "    model_config = {\n",
        "        'lstm_hidden': 64,\n",
        "        'hidden_units': [256, 128],\n",
        "        'dropout': 0.2\n",
        "    }\n",
        "    \n",
        "    model = TabularSeqModel(\n",
        "        d_features=len(feature_cols),\n",
        "        lstm_hidden=model_config['lstm_hidden'],\n",
        "        hidden_units=model_config['hidden_units'],\n",
        "        dropout=model_config['dropout']\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "    print(f\"Initial memory: {get_memory_usage()}\")\n",
        "    \n",
        "    # ê° splitì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ\n",
        "    for i, split_file in enumerate(split_files, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing Split {i}/{len(split_files)}: {os.path.basename(split_file)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        try:\n",
        "            # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "            split_df = load_and_downsample_data(split_file, CFG['DOWNSAMPLE_RATIO'])\n",
        "            \n",
        "            # í•™ìŠµ ìˆ˜í–‰\n",
        "            model = train_on_split(\n",
        "                model=model,\n",
        "                train_df=split_df,\n",
        "                feature_cols=feature_cols,\n",
        "                seq_col=seq_col,\n",
        "                target_col=target_col,\n",
        "                batch_size=CFG['BATCH_SIZE'],\n",
        "                epochs=CFG['EPOCHS_PER_SPLIT'],\n",
        "                lr=CFG['LEARNING_RATE'],\n",
        "                device=device\n",
        "            )\n",
        "            \n",
        "            # ì¤‘ê°„ ëª¨ë¸ ì €ì¥ (ì„ íƒì )\n",
        "            if i % 3 == 0:  # 3ë²ˆì§¸ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥\n",
        "                checkpoint_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_checkpoint_split_{i:02d}.pth\")\n",
        "                save_model(model, checkpoint_path, model_config)\n",
        "            \n",
        "            print(f\"Completed split {i}/{len(split_files)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing split {i}: {str(e)}\")\n",
        "            continue\n",
        "            \n",
        "        finally:\n",
        "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "            if 'split_df' in locals():\n",
        "                del split_df\n",
        "            clear_memory()\n",
        "            print(f\"Memory after cleanup: {get_memory_usage()}\")\n",
        "    \n",
        "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Saving Final Model\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    final_model_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_final.pth\")\n",
        "    save_model(model, final_model_path, model_config)\n",
        "    \n",
        "    print(f\"Training completed!\")\n",
        "    print(f\"Final model saved to: {final_model_path}\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "    return model, feature_cols\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started at: 2025-09-19 16:22:04.724861\n",
            "================================================================================\n",
            "ğŸš€ Starting Enhanced Split Data Training with Advanced Gradient Descent\n",
            "================================================================================\n",
            "ğŸ“ Found 10 split files:\n",
            "   - part_01.parquet\n",
            "   - part_02.parquet\n",
            "   - part_03.parquet\n",
            "   - part_04.parquet\n",
            "   - part_05.parquet\n",
            "   - part_06.parquet\n",
            "   - part_07.parquet\n",
            "   - part_08.parquet\n",
            "   - part_09.parquet\n",
            "   - part_10.parquet\n",
            "\n",
            "==================================================\n",
            "ğŸ” Analyzing first split for feature info...\n",
            "Loading data from: ../data/processed/split_data\\part_01.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61524, 119)\n",
            "Clicked=0: 41016, Clicked=1: 20508\n",
            "ğŸ“Š Number of features: 117\n",
            "ğŸ“ˆ Sequence column: seq\n",
            "ğŸ¯ Target column: clicked\n",
            "\n",
            "==================================================\n",
            "ğŸ§  Initializing model & optimization strategy...\n",
            "âœ… Model initialized with 97,003 parameters\n",
            "âš™ï¸  Optimizer: Adam with LR=1.0e-03, weight_decay=1e-5\n",
            "ğŸ“ˆ Scheduler: CosineAnnealingWarmRestarts\n",
            "ğŸ’¾ Initial memory: GPU Memory - Allocated: 0.4MB, Cached: 2.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 1/10: part_01.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_01.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61524, 119)\n",
            "Clicked=0: 41016, Clicked=1: 20508\n",
            "ğŸ”„ Training on Split 1 with 61524 samples\n",
            "ğŸ“Š Current learning rate: 0.001000\n",
            "   Train: 49219, Validation: 12305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 1] Train: 0.6361 | Val: 0.6328\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.2MB, Cached: 5500.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:16<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 2] Train: 0.6056 | Val: 0.6136\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.4MB, Cached: 8216.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 1 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:15<00:00,  1.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 1 Epoch 3] Train: 0.5997 | Val: 0.6024\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.2MB, Cached: 8216.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.001000 -> 0.000750\n",
            "âœ… Split 1 completed! Final validation loss: 0.6024\n",
            "âœ… Split 1/10 completed!\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 58.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 2/10: part_02.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_02.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (63441, 119)\n",
            "Clicked=0: 42294, Clicked=1: 21147\n",
            "ğŸ”„ Training on Split 2 with 63441 samples\n",
            "ğŸ“Š Current learning rate: 0.000750\n",
            "   Train: 50752, Validation: 12689\n",
            "ğŸ” Initial validation loss: 0.6002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:18<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 1] Train: 0.5900 | Val: 0.5907\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 35.0MB, Cached: 10454.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:20<00:00,  1.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 2] Train: 0.5845 | Val: 0.5849\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 33.7MB, Cached: 15594.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 2 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:21<00:00,  1.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 2 Epoch 3] Train: 0.5831 | Val: 0.5828\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 35.5MB, Cached: 15594.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000750 -> 0.000251\n",
            "âœ… Split 2 completed! Final validation loss: 0.5828\n",
            "âœ… Split 2/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +3.25%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 3/10: part_03.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_03.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (63387, 119)\n",
            "Clicked=0: 42258, Clicked=1: 21129\n",
            "ğŸ”„ Training on Split 3 with 63387 samples\n",
            "ğŸ“Š Current learning rate: 0.000251\n",
            "   Train: 50709, Validation: 12678\n",
            "ğŸ” Initial validation loss: 0.5835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:21<00:00, 10.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 1] Train: 0.5795 | Val: 0.5815\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 35.6MB, Cached: 10440.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:12<00:00, 10.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 2] Train: 0.5774 | Val: 0.5807\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 37.0MB, Cached: 10440.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 3 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:40<00:00,  7.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 3 Epoch 3] Train: 0.5777 | Val: 0.5803\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 37.3MB, Cached: 10440.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000251 -> 0.001000\n",
            "âœ… Split 3 completed! Final validation loss: 0.5803\n",
            "ğŸ’¾ Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_03.pth\n",
            "âœ… Split 3/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +0.42%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 62.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 4/10: part_04.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_04.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (57975, 119)\n",
            "Clicked=0: 38650, Clicked=1: 19325\n",
            "ğŸ”„ Training on Split 4 with 57975 samples\n",
            "ğŸ“Š Current learning rate: 0.001000\n",
            "   Train: 46380, Validation: 11595\n",
            "ğŸ” Initial validation loss: 0.5766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:34<00:00,  2.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 1] Train: 0.5820 | Val: 0.5765\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 31.1MB, Cached: 10350.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:39<00:00,  3.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 2] Train: 0.5794 | Val: 0.5769\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 31.4MB, Cached: 10350.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 4 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:39<00:00,  3.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 4 Epoch 3] Train: 0.5761 | Val: 0.5750\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 33.1MB, Cached: 10350.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.001000 -> 0.000750\n",
            "âœ… Split 4 completed! Final validation loss: 0.5750\n",
            "âœ… Split 4/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +0.92%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 58.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 5/10: part_05.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_05.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (57609, 119)\n",
            "Clicked=0: 38406, Clicked=1: 19203\n",
            "ğŸ”„ Training on Split 5 with 57609 samples\n",
            "ğŸ“Š Current learning rate: 0.000750\n",
            "   Train: 46087, Validation: 11522\n",
            "ğŸ” Initial validation loss: 0.5761\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:41<00:00,  3.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 1] Train: 0.5826 | Val: 0.5760\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.2MB, Cached: 13190.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:39<00:00,  3.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 2] Train: 0.5788 | Val: 0.5740\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.4MB, Cached: 15840.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 5 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:37<00:00,  3.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 5 Epoch 3] Train: 0.5777 | Val: 0.5738\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 29.2MB, Cached: 15840.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000750 -> 0.000251\n",
            "âœ… Split 5 completed! Final validation loss: 0.5738\n",
            "âœ… Split 5/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +0.21%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 6/10: part_06.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_06.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (61506, 119)\n",
            "Clicked=0: 41004, Clicked=1: 20502\n",
            "ğŸ”„ Training on Split 6 with 61506 samples\n",
            "ğŸ“Š Current learning rate: 0.000251\n",
            "   Train: 49204, Validation: 12302\n",
            "ğŸ” Initial validation loss: 0.5743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:35<00:00,  2.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 1] Train: 0.5710 | Val: 0.5757\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 9406.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:26<00:00,  2.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 2] Train: 0.5754 | Val: 0.5746\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 9406.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 6 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:25<00:00,  1.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 6 Epoch 3] Train: 0.5754 | Val: 0.5745\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 18.1MB, Cached: 12058.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000251 -> 0.001000\n",
            "âœ… Split 6 completed! Final validation loss: 0.5745\n",
            "ğŸ’¾ Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_06.pth\n",
            "âœ… Split 6/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: -0.12%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 7/10: part_07.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_07.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (62148, 119)\n",
            "Clicked=0: 41432, Clicked=1: 20716\n",
            "ğŸ”„ Training on Split 7 with 62148 samples\n",
            "ğŸ“Š Current learning rate: 0.001000\n",
            "   Train: 49718, Validation: 12430\n",
            "ğŸ” Initial validation loss: 0.5585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:18<00:00,  1.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 1] Train: 0.5773 | Val: 0.5590\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 23.9MB, Cached: 7876.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [01:46<00:00,  8.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 2] Train: 0.5777 | Val: 0.5612\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 24.2MB, Cached: 10460.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 7 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [02:56<00:00, 13.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 7 Epoch 3] Train: 0.5731 | Val: 0.5584\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 23.1MB, Cached: 10460.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.001000 -> 0.000750\n",
            "âœ… Split 7 completed! Final validation loss: 0.5584\n",
            "âœ… Split 7/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +2.80%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 8/10: part_08.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_08.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (62640, 119)\n",
            "Clicked=0: 41760, Clicked=1: 20880\n",
            "ğŸ”„ Training on Split 8 with 62640 samples\n",
            "ğŸ“Š Current learning rate: 0.000750\n",
            "   Train: 50112, Validation: 12528\n",
            "ğŸ” Initial validation loss: 0.5577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:32<00:00,  2.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 1] Train: 0.5766 | Val: 0.5554\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.8MB, Cached: 9914.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:32<00:00,  2.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 2] Train: 0.5734 | Val: 0.5554\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.7MB, Cached: 14798.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 8 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:24<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 8 Epoch 3] Train: 0.5723 | Val: 0.5537\n",
            "   LR: 0.000750 | Memory: GPU Memory - Allocated: 28.5MB, Cached: 4654.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000750 -> 0.000251\n",
            "âœ… Split 8 completed! Final validation loss: 0.5537\n",
            "âœ… Split 8/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: +0.84%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 9/10: part_09.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_09.parquet\n",
            "Original shape: (1070417, 119)\n",
            "Downsampled shape: (60402, 119)\n",
            "Clicked=0: 40268, Clicked=1: 20134\n",
            "ğŸ”„ Training on Split 9 with 60402 samples\n",
            "ğŸ“Š Current learning rate: 0.000251\n",
            "   Train: 48321, Validation: 12081\n",
            "ğŸ” Initial validation loss: 0.5718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:51<00:00,  9.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 1] Train: 0.5751 | Val: 0.5726\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 62.6MB, Cached: 15714.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:14<00:00, 21.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 2] Train: 0.5732 | Val: 0.5715\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 55.9MB, Cached: 15714.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 9 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [04:28<00:00, 22.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 9 Epoch 3] Train: 0.5735 | Val: 0.5713\n",
            "   LR: 0.000251 | Memory: GPU Memory - Allocated: 62.6MB, Cached: 18366.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.000251 -> 0.001000\n",
            "âœ… Split 9 completed! Final validation loss: 0.5713\n",
            "ğŸ’¾ Checkpoint saved: ../models/ctr_lstm_mlp_enhanced_checkpoint_split_09.pth\n",
            "âœ… Split 9/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: -3.18%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ Processing Split 10/10: part_10.parquet\n",
            "================================================================================\n",
            "Loading data from: ../data/processed/split_data\\part_10.parquet\n",
            "Original shape: (1064179, 119)\n",
            "Downsampled shape: (61575, 119)\n",
            "Clicked=0: 41050, Clicked=1: 20525\n",
            "ğŸ”„ Training on Split 10 with 61575 samples\n",
            "ğŸ“Š Current learning rate: 0.001000\n",
            "   Train: 49260, Validation: 12315\n",
            "ğŸ” Initial validation loss: 0.5921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:24<00:00,  1.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 1] Train: 0.5700 | Val: 0.5916\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.6MB, Cached: 11880.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:30<00:00,  2.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 2] Train: 0.5695 | Val: 0.5884\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.7MB, Cached: 11880.0MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Split 10 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:35<00:00,  2.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   [Split 10 Epoch 3] Train: 0.5718 | Val: 0.5949\n",
            "   LR: 0.001000 | Memory: GPU Memory - Allocated: 18.7MB, Cached: 11880.0MB\n",
            "   ğŸ“ˆ LR Scheduler: 0.001000 -> 0.000750\n",
            "âœ… Split 10 completed! Final validation loss: 0.5949\n",
            "âœ… Split 10/10 completed!\n",
            "ğŸ“ˆ Validation Loss Change: -4.13%\n",
            "ğŸ§¹ Memory after cleanup: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š Training Summary\n",
            "================================================================================\n",
            "Split-wise Performance:\n",
            "   Split  1: Val Loss = 0.6024, LR = 0.000750\n",
            "   Split  2: Val Loss = 0.5828, LR = 0.000251\n",
            "   Split  3: Val Loss = 0.5803, LR = 0.001000\n",
            "   Split  4: Val Loss = 0.5750, LR = 0.000750\n",
            "   Split  5: Val Loss = 0.5738, LR = 0.000251\n",
            "   Split  6: Val Loss = 0.5745, LR = 0.001000\n",
            "   Split  7: Val Loss = 0.5584, LR = 0.000750\n",
            "   Split  8: Val Loss = 0.5537, LR = 0.000251\n",
            "   Split  9: Val Loss = 0.5713, LR = 0.001000\n",
            "   Split 10: Val Loss = 0.5949, LR = 0.000750\n",
            "\n",
            "ğŸ¯ Overall Improvement: +1.24%\n",
            "ğŸ“ˆ Initial Loss: 0.6024 â†’ Final Loss: 0.5949\n",
            "\n",
            "============================================================\n",
            "ğŸ’¾ Saving Final Model with Full State\n",
            "============================================================\n",
            "âœ… Training completed successfully!\n",
            "ğŸ’¾ Final model saved to: ../models/ctr_lstm_mlp_enhanced_final.pth\n",
            "ğŸ§  Final memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n",
            "ğŸ¯ Final learning rate: 0.000750\n",
            "\n",
            "Training completed!\n",
            "Training time: 0:38:14.215847\n",
            "Final memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë“  split ë°ì´í„°ë¡œ í•™ìŠµ ì‹¤í–‰\n",
        "start_time = datetime.now()\n",
        "print(f\"Training started at: {start_time}\")\n",
        "\n",
        "try:\n",
        "    trained_model, feature_columns = train_all_splits()\n",
        "    \n",
        "    end_time = datetime.now()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Training time: {training_time}\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Training failed with error: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n",
            "Test data shape: (1527298, 119)\n",
            "Test data columns: ['ID', 'gender', 'age_group', 'inventory_id', 'day_of_week', 'hour', 'seq', 'l_feat_1', 'l_feat_2', 'l_feat_3', 'l_feat_4', 'l_feat_5', 'l_feat_6', 'l_feat_7', 'l_feat_8', 'l_feat_9', 'l_feat_10', 'l_feat_11', 'l_feat_12', 'l_feat_13', 'l_feat_14', 'l_feat_15', 'l_feat_16', 'l_feat_17', 'l_feat_18', 'l_feat_19', 'l_feat_20', 'l_feat_21', 'l_feat_22', 'l_feat_23', 'l_feat_24', 'l_feat_25', 'l_feat_26', 'l_feat_27', 'feat_e_1', 'feat_e_2', 'feat_e_3', 'feat_e_4', 'feat_e_5', 'feat_e_6', 'feat_e_7', 'feat_e_8', 'feat_e_9', 'feat_e_10', 'feat_d_1', 'feat_d_2', 'feat_d_3', 'feat_d_4', 'feat_d_5', 'feat_d_6', 'feat_c_1', 'feat_c_2', 'feat_c_3', 'feat_c_4', 'feat_c_5', 'feat_c_6', 'feat_c_7', 'feat_c_8', 'feat_b_1', 'feat_b_2', 'feat_b_3', 'feat_b_4', 'feat_b_5', 'feat_b_6', 'feat_a_1', 'feat_a_2', 'feat_a_3', 'feat_a_4', 'feat_a_5', 'feat_a_6', 'feat_a_7', 'feat_a_8', 'feat_a_9', 'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_14', 'feat_a_15', 'feat_a_16', 'feat_a_17', 'feat_a_18', 'history_a_1', 'history_a_2', 'history_a_3', 'history_a_4', 'history_a_5', 'history_a_6', 'history_a_7', 'history_b_1', 'history_b_2', 'history_b_3', 'history_b_4', 'history_b_5', 'history_b_6', 'history_b_7', 'history_b_8', 'history_b_9', 'history_b_10', 'history_b_11', 'history_b_12', 'history_b_13', 'history_b_14', 'history_b_15', 'history_b_16', 'history_b_17', 'history_b_18', 'history_b_19', 'history_b_20', 'history_b_21', 'history_b_22', 'history_b_23', 'history_b_24', 'history_b_25', 'history_b_26', 'history_b_27', 'history_b_28', 'history_b_29', 'history_b_30']\n",
            "Test data shape after removing ID: (1527298, 118)\n",
            "Memory usage: GPU Memory - Allocated: 17.7MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "print(\"Loading test data...\")\n",
        "test_df = pd.read_parquet(\"../data/raw/test.parquet\", engine=\"pyarrow\")\n",
        "\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"Test data columns:\", test_df.columns.tolist())\n",
        "\n",
        "# ID ì»¬ëŸ¼ ë”°ë¡œ ë³´ê´€ (ì œì¶œìš©)\n",
        "test_ids = test_df['ID'].copy()\n",
        "\n",
        "# ID ì»¬ëŸ¼ ì œê±°\n",
        "test_df = test_df.drop(columns=['ID'])\n",
        "\n",
        "print(f\"Test data shape after removing ID: {test_df.shape}\")\n",
        "print(f\"Memory usage: {get_memory_usage()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with Saved Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_inference(model_path, test_df, feature_cols, batch_size=4096):\n",
        "    \"\"\"\n",
        "    ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    \n",
        "    # ëª¨ë¸ ë¡œë“œ\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file not found: {model_path}\")\n",
        "        return None\n",
        "        \n",
        "    model = load_model(model_path, len(feature_cols), device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded successfully!\")\n",
        "    print(f\"Memory after model loading: {get_memory_usage()}\")\n",
        "    \n",
        "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
        "    seq_col = \"seq\"\n",
        "    test_dataset = ClickDataset(test_df, feature_cols, seq_col, has_target=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_infer)\n",
        "    \n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "    print(f\"Number of batches: {len(test_loader)}\")\n",
        "    \n",
        "    # ì¶”ë¡  ìˆ˜í–‰\n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (xs, seqs, lens) in enumerate(tqdm(test_loader, desc=\"Inference\")):\n",
        "            xs, seqs, lens = xs.to(device), seqs.to(device), lens.to(device)\n",
        "            \n",
        "            # ëª¨ë¸ ì˜ˆì¸¡\n",
        "            logits = model(xs, seqs, lens)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            \n",
        "            predictions.append(probs.cpu())\n",
        "            \n",
        "            # ë©”ëª¨ë¦¬ ì •ë¦¬ (í° ë°°ì¹˜ ì²˜ë¦¬ ì‹œ)\n",
        "            if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "                clear_memory()\n",
        "    \n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ í•©ì¹˜ê¸°\n",
        "    final_predictions = torch.cat(predictions).numpy()\n",
        "    \n",
        "    print(f\"Inference completed!\")\n",
        "    print(f\"Predictions shape: {final_predictions.shape}\")\n",
        "    print(f\"Prediction range: [{final_predictions.min():.4f}, {final_predictions.max():.4f}]\")\n",
        "    print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "    del model\n",
        "    clear_memory()\n",
        "    \n",
        "    return final_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ”® STARTING ENHANCED INFERENCE WITH TRAINED MODEL\n",
            "================================================================================\n",
            "âœ… Using feature columns from training session: 117 features\n",
            "ğŸ¯ Final feature count: 117\n",
            "============================================================\n",
            "ğŸ”® Starting Model Inference\n",
            "============================================================\n",
            "ğŸ”„ Loading model from: ../models/ctr_lstm_mlp_enhanced_final.pth\n",
            "ğŸ“Š Features from checkpoint: 117\n",
            "ğŸ—ï¸  Model config: LSTM=64, Hidden=[256, 128]\n",
            "â° Model timestamp: 2025-09-19T17:00:18.930169\n",
            "ğŸ“š Trained on 10 splits\n",
            "ğŸ¯ Final validation loss: 0.5949\n",
            "âœ… Model loaded successfully!\n",
            "âœ… Using feature columns from checkpoint: 117 features\n",
            "ğŸ’¾ Memory after model loading: GPU Memory - Allocated: 17.4MB, Cached: 60.0MB\n",
            "ğŸ“Š Test dataset size: 1,527,298\n",
            "ğŸ“¦ Number of batches: 373\n",
            "ğŸ”§ Batch size: 4,096\n",
            "\n",
            "ğŸš€ Starting inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ”® Inference Progress:  27%|â–ˆâ–ˆâ–‹       | 101/373 [01:36<04:17,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ğŸ“Š Processed 100 batches | Avg batch time: 0.143s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ”® Inference Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 201/373 [03:10<02:53,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ğŸ“Š Processed 200 batches | Avg batch time: 0.143s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ”® Inference Progress:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 301/373 [04:42<01:08,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ğŸ“Š Processed 300 batches | Avg batch time: 0.137s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ”® Inference Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 373/373 [05:46<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Inference completed successfully!\n",
            "â±ï¸  Total inference time: 346.79 seconds\n",
            "ğŸ“Š Average batch time: 0.141 seconds\n",
            "ğŸ”¢ Predictions shape: (1527298,)\n",
            "ğŸ“ˆ Prediction statistics:\n",
            "   Min: 0.004356\n",
            "   Max: 1.000000\n",
            "   Mean: 0.304236\n",
            "   Std: 0.131825\n",
            "   Median: 0.291847\n",
            "ğŸ“Š Prediction distribution:\n",
            "   0.0-0.1: 27,535 (1.8%)\n",
            "   0.1-0.2: 363,634 (23.8%)\n",
            "   0.2-0.3: 407,705 (26.7%)\n",
            "   0.3-0.4: 377,544 (24.7%)\n",
            "   0.4-0.5: 216,029 (14.1%)\n",
            "   0.5-0.6: 102,595 (6.7%)\n",
            "   0.6-0.7: 27,081 (1.8%)\n",
            "   0.7-0.8: 4,295 (0.3%)\n",
            "   0.8-0.9: 698 (0.0%)\n",
            "   0.9-1.0: 182 (0.0%)\n",
            "ğŸ’¾ Final memory usage: GPU Memory - Allocated: 71.2MB, Cached: 3230.0MB\n"
          ]
        }
      ],
      "source": [
        "# ì €ì¥ëœ ëª¨ë¸ë¡œ ì¶”ë¡  ìˆ˜í–‰\n",
        "final_model_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_final.pth\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Starting Inference\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# í•™ìŠµì´ ì™„ë£Œëœ ê²½ìš° feature_columns ì‚¬ìš©, ì•„ë‹ˆë©´ ëª¨ë¸ì—ì„œ ì¶”ì¶œ\n",
        "if 'feature_columns' in locals() and feature_columns is not None:\n",
        "    inference_feature_cols = feature_columns\n",
        "else:\n",
        "    # ì²« ë²ˆì§¸ split íŒŒì¼ì—ì„œ feature ì •ë³´ ì¶”ì¶œ\n",
        "    print(\"Extracting feature information from split data...\")\n",
        "    split_files = sorted(glob.glob(os.path.join(CFG['SPLIT_DATA_PATH'], \"part_*.parquet\")))\n",
        "    if len(split_files) > 0:\n",
        "        temp_df = pd.read_parquet(split_files[0], engine=\"pyarrow\", nrows=1000)  # ì‘ì€ ìƒ˜í”Œë§Œ\n",
        "        inference_feature_cols = get_feature_columns(temp_df)\n",
        "        del temp_df\n",
        "        clear_memory()\n",
        "    else:\n",
        "        print(\"Error: No split files found for feature extraction!\")\n",
        "        raise FileNotFoundError(\"Split files not found\")\n",
        "\n",
        "print(f\"Using {len(inference_feature_cols)} features for inference\")\n",
        "\n",
        "# ì¶”ë¡  ì‹¤í–‰\n",
        "test_predictions = perform_inference(\n",
        "    model_path=final_model_path,\n",
        "    test_df=test_df,\n",
        "    feature_cols=inference_feature_cols,\n",
        "    batch_size=CFG['BATCH_SIZE']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ğŸ“‹ Creating Submission File (ë² ì´ìŠ¤ë¼ì¸ í˜¸í™˜ ì–‘ì‹)\n",
            "============================================================\n",
            "Sample submission shape: (1527298, 2)\n",
            "Submission predictions stats:\n",
            "  Min: 0.004356\n",
            "  Max: 1.000000\n",
            "  Mean: 0.304236\n",
            "  Std: 0.131825\n",
            "ğŸ“ Found 2 existing submission files. Next: submission_3.csv\n",
            "ğŸ’¾ Submission file saved: ../outputs\\submission_3.csv\n",
            "ğŸ“Š Submission shape: (1527298, 2)\n",
            "ğŸ“ File: submission_3.csv\n",
            "ğŸ§¹ Final memory usage: GPU Memory - Allocated: 17.0MB, Cached: 60.0MB\n"
          ]
        }
      ],
      "source": [
        "if test_predictions is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ“‹ Creating Submission File (ë² ì´ìŠ¤ë¼ì¸ í˜¸í™˜ ì–‘ì‹)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # ê¸°ì¡´ sample_submission.csv íŒŒì¼ ì½ê¸°\n",
        "    sample_submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
        "    print(f\"Sample submission shape: {sample_submission.shape}\")\n",
        "    \n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ë¡œ clicked ì»¬ëŸ¼ ì—…ë°ì´íŠ¸\n",
        "    submission_df = sample_submission.copy()\n",
        "    submission_df['clicked'] = test_predictions\n",
        "    \n",
        "    # ê²°ê³¼ í™•ì¸\n",
        "    print(f\"Submission predictions stats:\")\n",
        "    print(f\"  Min: {test_predictions.min():.6f}\")\n",
        "    print(f\"  Max: {test_predictions.max():.6f}\")\n",
        "    print(f\"  Mean: {test_predictions.mean():.6f}\")\n",
        "    print(f\"  Std: {test_predictions.std():.6f}\")\n",
        "    \n",
        "    # outputs í´ë” í™•ì¸ ë° ìƒì„±\n",
        "    output_dir = '../outputs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # ê¸°ì¡´ ì œì¶œ íŒŒì¼ í™•ì¸í•˜ì—¬ ë²ˆí˜¸ ê²°ì • (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹)\n",
        "    existing_files = [f for f in os.listdir(output_dir) if f.startswith('submission_') and f.endswith('.csv')]\n",
        "    \n",
        "    if len(existing_files) == 0:\n",
        "        next_num = 1\n",
        "        print(\"ğŸ“ No existing submission files found. Starting with submission_1.csv\")\n",
        "    else:\n",
        "        nums = [int(f.split('_')[1].split('.')[0]) for f in existing_files]\n",
        "        next_num = max(nums) + 1\n",
        "        print(f\"ğŸ“ Found {len(existing_files)} existing submission files. Next: submission_{next_num}.csv\")\n",
        "    \n",
        "    # ìƒˆë¡œìš´ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ë² ì´ìŠ¤ë¼ì¸ê³¼ ë™ì¼í•œ ì–‘ì‹)\n",
        "    output_path = os.path.join(output_dir, f'submission_{next_num}.csv')\n",
        "    \n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"ğŸ’¾ Submission file saved: {output_path}\")\n",
        "    print(f\"ğŸ“Š Submission shape: {submission_df.shape}\")\n",
        "    print(f\"ğŸ“ File: submission_{next_num}.csv\")\n",
        "    \n",
        "    # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "    clear_memory()\n",
        "    print(f\"ğŸ§¹ Final memory usage: {get_memory_usage()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Error: No predictions available for submission!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "\n",
        "### ğŸ“Š **ì£¼ìš” ê¸°ëŠ¥**\n",
        "1. **Split ë°ì´í„° ìˆœì°¨ ì²˜ë¦¬**: 10ê°œë¡œ ë¶„í• ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ìˆœì°¨ í•™ìŠµ\n",
        "2. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ê° split ì²˜ë¦¬ í›„ ìë™ ë©”ëª¨ë¦¬ í•´ì œ ë° ëª¨ë‹ˆí„°ë§\n",
        "3. **ì¦ë¶„ í•™ìŠµ**: ì´ì „ í•™ìŠµ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ split ë°ì´í„° í•™ìŠµ\n",
        "4. **ëª¨ë¸ ì €ì¥**: models í´ë”ì— ì²´í¬í¬ì¸íŠ¸ ë° ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "5. **ìë™í™”ëœ ì¶”ë¡ **: ì €ì¥ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "\n",
        "### ğŸ”§ **ê¸°ìˆ ì  íŠ¹ì§•**\n",
        "- **LSTM + MLP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸**: ì‹œí€€ìŠ¤ì™€ í…Œì´ë¸” í”¼ì²˜ë¥¼ ë™ì‹œ ì²˜ë¦¬\n",
        "- **ë‹¤ìš´ìƒ˜í”Œë§**: ë¶ˆê· í˜• ë°ì´í„° í•´ê²° (clicked=0 : clicked=1 = 2:1 ë¹„ìœ¨)\n",
        "- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ìœ„í•œ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬\n",
        "- **GPU ë©”ëª¨ë¦¬ ìµœì í™”**: CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ë° ëª¨ë‹ˆí„°ë§\n",
        "\n",
        "### ğŸ“ **ì¶œë ¥ íŒŒì¼**\n",
        "- **ëª¨ë¸**: `../models/ctr_lstm_mlp_model_final.pth`\n",
        "- **ì²´í¬í¬ì¸íŠ¸**: `../models/ctr_lstm_mlp_model_checkpoint_split_XX.pth` (3ë²ˆì§¸ë§ˆë‹¤)\n",
        "- **ì œì¶œ íŒŒì¼**: `../outputs/submission_split_training_XXX_YYYYMMDD_HHMMSS.csv`\n",
        "\n",
        "### âš¡ **ì„±ëŠ¥ ìµœì í™”**\n",
        "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\n",
        "- ê° split ì²˜ë¦¬ í›„ ìë™ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
        "- ë°°ì¹˜ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬ (100ë°°ì¹˜ë§ˆë‹¤)\n",
        "- PyTorch CUDA ìºì‹œ ì •ë¦¬\n",
        "\n",
        "ì´ ë°©ì‹ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë„ ë©”ëª¨ë¦¬ ì œí•œ ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dacon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

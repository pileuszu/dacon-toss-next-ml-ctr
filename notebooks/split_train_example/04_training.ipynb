{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Enhanced Split Training\n",
        "\n",
        "## 개요\n",
        "10개로 분할된 데이터를 Enhanced Gradient Descent 전략으로 순차 학습하는 노트북입니다.\n",
        "\n",
        "## 주요 특징\n",
        "- **Advanced Gradient Descent**: 전역 optimizer & scheduler 상태 유지\n",
        "- **Catastrophic Forgetting 방지**: gradient clipping + 성능 감소 감지\n",
        "- **메모리 효율성**: 각 split 처리 후 자동 메모리 관리\n",
        "- **체크포인트 시스템**: 중간 저장 및 완전한 상태 복원\n",
        "\n",
        "**주의**: 이 노트북을 실행하기 전에 **01**, **02**, **03** 노트북을 먼저 실행해주세요!\n",
        "\n",
        "**시간 소요**: 약 30분~2시간 (GPU 성능에 따라)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Split Training Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_on_split(model, train_df, feature_cols, seq_col, target_col, optimizer, scheduler, \n",
        "                   split_idx, batch_size=4096, epochs=3, device=\"cuda\", prev_val_loss=None):\n",
        "    \"\"\"\n",
        "    Enhanced Gradient Descent를 적용한 단일 split 학습 함수\n",
        "    \n",
        "    Args:\n",
        "        model: 학습할 모델\n",
        "        train_df: 학습 데이터 \n",
        "        feature_cols: 피처 컬럼들\n",
        "        seq_col: 시퀀스 컬럼\n",
        "        target_col: 타겟 컬럼\n",
        "        optimizer: 전역 optimizer (상태 유지)\n",
        "        scheduler: 전역 scheduler (상태 유지)\n",
        "        split_idx: 현재 split 번호\n",
        "        prev_val_loss: 이전 split의 validation loss (catastrophic forgetting 감지용)\n",
        "    \"\"\"\n",
        "    print(f\"Training on Split {split_idx} with {len(train_df)} samples\")\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Train/Validation split\n",
        "    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
        "    print(f\"Train: {len(tr_df)}, Validation: {len(va_df)}\")\n",
        "\n",
        "    # Dataset & DataLoader\n",
        "    train_dataset = ClickDataset(tr_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "    val_dataset = ClickDataset(va_df, feature_cols, seq_col, target_col, has_target=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Early stopping parameters\n",
        "    patience = 2\n",
        "    patience_counter = 0\n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    # Training history for this split\n",
        "    split_history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
        "\n",
        "    # Training Loop with Enhanced Gradient Descent\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_batches = 0\n",
        "        \n",
        "        train_progress = tqdm(train_loader, desc=f\"Split {split_idx} Epoch {epoch}\")\n",
        "        for xs, seqs, seq_lens, ys in train_progress:\n",
        "            xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xs, seqs, seq_lens)\n",
        "            loss = criterion(logits, ys)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient Clipping (catastrophic forgetting 방지)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG['GRADIENT_CLIP_NORM'])\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "            \n",
        "            # Progress bar 업데이트\n",
        "            train_progress.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'lr': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            })\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for xs, seqs, seq_lens, ys in tqdm(val_loader, desc=f\"Validation {epoch}\"):\n",
        "                xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
        "                \n",
        "                logits = model(xs, seqs, seq_lens)\n",
        "                loss = criterion(logits, ys)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        print(f\"[Split {split_idx} Epoch {epoch}] Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}\")\n",
        "        print(f\"LR: {current_lr:.6f} | Memory: {get_memory_usage()}\")\n",
        "        \n",
        "        # History 저장\n",
        "        split_history['train_loss'].append(avg_train_loss)\n",
        "        split_history['val_loss'].append(avg_val_loss)\n",
        "        split_history['lr'].append(current_lr)\n",
        "        \n",
        "        # Early stopping & adaptive LR\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= patience:\n",
        "            # LR 감소\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= CFG['LR_REDUCTION_FACTOR']\n",
        "            print(f\"Learning rate reduced to: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            patience_counter = 0\n",
        "        \n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "    # Catastrophic forgetting 감지\n",
        "    final_val_loss = avg_val_loss\n",
        "    catastrophic_forgetting = False\n",
        "    \n",
        "    if prev_val_loss is not None and final_val_loss > prev_val_loss * CFG['CATASTROPHIC_THRESHOLD']:\n",
        "        print(f\"WARNING: Possible catastrophic forgetting detected!\")\n",
        "        print(f\"Previous val loss: {prev_val_loss:.4f} -> Current: {final_val_loss:.4f}\")\n",
        "        \n",
        "        # LR 감소로 대응\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] *= CFG['LR_REDUCTION_FACTOR']\n",
        "        print(f\"Learning rate reduced to: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        catastrophic_forgetting = True\n",
        "\n",
        "    return model, final_val_loss, split_history, catastrophic_forgetting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split 파일 목록 가져오기\n",
        "print(\"=\"*80)\n",
        "print(\"STARTING ENHANCED SPLIT DATA TRAINING WITH ADVANCED GRADIENT DESCENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "split_files = sorted(glob.glob(os.path.join(CFG['SPLIT_DATA_PATH'], \"part_*.parquet\")))\n",
        "print(f\"Found {len(split_files)} split files:\")\n",
        "for f in split_files:\n",
        "    print(f\"  - {os.path.basename(f)}\")\n",
        "\n",
        "if len(split_files) == 0:\n",
        "    print(\"No split files found!\")\n",
        "    raise FileNotFoundError(\"No split data files found in the specified directory\")\n",
        "\n",
        "# 첫 번째 파일로 feature 정보 확인\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Analyzing first split for feature info...\")\n",
        "first_df = load_and_downsample_data(split_files[0], CFG['DOWNSAMPLE_RATIO'])\n",
        "feature_cols = get_feature_columns(first_df)\n",
        "seq_col = \"seq\"\n",
        "target_col = \"clicked\"\n",
        "\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Sequence column: {seq_col}\")\n",
        "print(f\"Target column: {target_col}\")\n",
        "\n",
        "# 메모리 정리\n",
        "del first_df\n",
        "clear_memory()\n",
        "\n",
        "# 모델 초기화\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Initializing model & optimization strategy...\")\n",
        "model_config = {\n",
        "    'lstm_hidden': 64,\n",
        "    'hidden_units': [256, 128],\n",
        "    'dropout': 0.2\n",
        "}\n",
        "\n",
        "model = TabularSeqModel(\n",
        "    d_features=len(feature_cols),\n",
        "    lstm_hidden=model_config['lstm_hidden'],\n",
        "    hidden_units=model_config['hidden_units'],\n",
        "    dropout=model_config['dropout']\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Gradient Descent 전략 설정\n",
        "print(\"Setting up Enhanced Gradient Descent strategy...\")\n",
        "\n",
        "# 전역 Optimizer (모든 split에서 상태 유지)\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=CFG['LEARNING_RATE'], \n",
        "    weight_decay=CFG['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# 전역 Scheduler (모든 split에서 상태 유지)  \n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, \n",
        "    T_0=10,  # 첫 번째 restart까지의 epoch 수\n",
        "    T_mult=2,  # restart 간격 증가 배수\n",
        "    eta_min=1e-6  # 최소 학습률\n",
        ")\n",
        "\n",
        "print(f\"Optimizer: Adam with LR={CFG['LEARNING_RATE']:.0e}, weight_decay={CFG['WEIGHT_DECAY']}\")\n",
        "print(f\"Scheduler: CosineAnnealingWarmRestarts\")\n",
        "print(f\"Initial memory: {get_memory_usage()}\")\n",
        "\n",
        "# 학습 이력 저장을 위한 변수들\n",
        "global_training_history = {\n",
        "    'splits': [],\n",
        "    'train_losses': [], \n",
        "    'val_losses': [],\n",
        "    'learning_rates': [],\n",
        "    'catastrophic_forgetting_events': 0\n",
        "}\n",
        "\n",
        "prev_val_loss = None\n",
        "start_time = datetime.now()\n",
        "print(f\"Training started at: {start_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Training Loop Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 각 split에 대해 순차적으로 학습\n",
        "for i, split_file in enumerate(split_files, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing Split {i}/{len(split_files)}: {os.path.basename(split_file)}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    try:\n",
        "        # 데이터 로드 및 전처리\n",
        "        split_df = load_and_downsample_data(split_file, CFG['DOWNSAMPLE_RATIO'])\n",
        "        \n",
        "        # Enhanced Gradient Descent로 학습 수행\n",
        "        model, current_val_loss, split_history, cf_detected = train_on_split(\n",
        "            model=model,\n",
        "            train_df=split_df,\n",
        "            feature_cols=feature_cols,\n",
        "            seq_col=seq_col,\n",
        "            target_col=target_col,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            split_idx=i,\n",
        "            batch_size=CFG['BATCH_SIZE'],\n",
        "            epochs=CFG['EPOCHS_PER_SPLIT'],\n",
        "            device=device,\n",
        "            prev_val_loss=prev_val_loss\n",
        "        )\n",
        "        \n",
        "        # 학습 이력 업데이트\n",
        "        global_training_history['splits'].append(i)\n",
        "        global_training_history['train_losses'].extend(split_history['train_loss'])\n",
        "        global_training_history['val_losses'].extend(split_history['val_loss'])\n",
        "        global_training_history['learning_rates'].extend(split_history['lr'])\n",
        "        \n",
        "        if cf_detected:\n",
        "            global_training_history['catastrophic_forgetting_events'] += 1\n",
        "        \n",
        "        # 중간 체크포인트 저장 (N번째 split마다)\n",
        "        if i % CFG['SAVE_CHECKPOINT_EVERY'] == 0:\n",
        "            checkpoint_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_enhanced_checkpoint_split_{i:02d}.pth\")\n",
        "            save_model(\n",
        "                model=model, \n",
        "                model_path=checkpoint_path, \n",
        "                model_config=model_config,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=scheduler,\n",
        "                feature_cols=feature_cols,\n",
        "                training_history=global_training_history\n",
        "            )\n",
        "            print(f\"Checkpoint saved: {os.path.basename(checkpoint_path)}\")\n",
        "        \n",
        "        # 이전 validation loss 업데이트\n",
        "        prev_val_loss = current_val_loss\n",
        "        \n",
        "        print(f\"Split {i}/{len(split_files)} completed successfully!\")\n",
        "        print(f\"Final validation loss: {current_val_loss:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR processing split {i}: {str(e)}\")\n",
        "        print(\"Continuing with next split...\")\n",
        "        continue\n",
        "        \n",
        "    finally:\n",
        "        # 메모리 정리\n",
        "        if 'split_df' in locals():\n",
        "            del split_df\n",
        "        clear_memory()\n",
        "        print(f\"Memory after cleanup: {get_memory_usage()}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ALL SPLITS COMPLETED!\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 최종 모델 저장\n",
        "print(\"Saving final enhanced model...\")\n",
        "final_model_path = os.path.join(CFG['MODELS_PATH'], f\"{CFG['MODEL_NAME']}_enhanced_final.pth\")\n",
        "\n",
        "save_model(\n",
        "    model=model,\n",
        "    model_path=final_model_path,\n",
        "    model_config=model_config,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    feature_cols=feature_cols,\n",
        "    training_history=global_training_history\n",
        ")\n",
        "\n",
        "# 학습 완료 요약\n",
        "end_time = datetime.now()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ENHANCED SPLIT TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Total training time: {total_time}\")\n",
        "print(f\"Number of splits processed: {len(split_files)}\")\n",
        "print(f\"Final model saved to: {final_model_path}\")\n",
        "print(f\"Catastrophic forgetting events: {global_training_history['catastrophic_forgetting_events']}\")\n",
        "print(f\"Final learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "print(f\"Final validation loss: {prev_val_loss:.4f}\")\n",
        "print(f\"Final memory usage: {get_memory_usage()}\")\n",
        "\n",
        "# 학습 완료 상태를 변수로 저장 (05번 노트북에서 사용)\n",
        "training_completed = True\n",
        "trained_model = model\n",
        "trained_feature_cols = feature_cols\n",
        "\n",
        "print(f\"\\nTraining variables ready for inference:\")\n",
        "print(f\"- training_completed: {training_completed}\")\n",
        "print(f\"- trained_model: Available\")\n",
        "print(f\"- trained_feature_cols: {len(trained_feature_cols)} features\")\n",
        "print(f\"\\nYou can now run 05_inference.ipynb for predictions!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
